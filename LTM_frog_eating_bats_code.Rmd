---
title: "Long term memory in frog-eating bats"
author: "M. May Dixon"
date: "3/3/2022"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

**Note: in the data, groups have different names than we used in the manuscript
*trained* = experienced bats
*control* = naive bats

*primary* = trained ringtone (A and B)
*control* = extinguished ringtone (E)
*static* = control sound

There were also a number of bats we don't include in the final dataset, becausec they have all kinds of different experiences. 
LTM.partial = bats I trained to both A and B for another experiment, never taught to discriminate (never heard E or other call they didn't get rewarded for), released and recaught
control.partial = control bats tested in preliminary trials, before we added the control sound/ static stimulus. They were played A, B, and the extinguished sound (confusingly, "control" in this dataset)

random.experience = oddball bats, with various histories in various experiments. Hard to interpret, have been trained to different things over different periods of times. Interesting anecdotes perhaps. 


## Install Packages
```{r}
library(tidyverse)
library(tidyverse)
library(boot)
library(cowplot)
library(lubridate)
library(beepr)
```

## Import Data
```{r}

LTM <- read_csv("https://raw.githubusercontent.com/maydixon/LTM/main/Long-term%20memory%20experiment_nocomments.csv")


head(LTM)

```


## Filter and reshape data
```{r}


LTM1 <- LTM %>% 
    filter(Group %in% c("control", "treatment")) %>% #exclude bats with odd  experiences
    pivot_longer(
        !c(Bat.ID:Days.Between),
        names_to = "Playback",
        names_prefix = "Response.to.", #remove string at start of playback
        values_to = "Response.score",
        names_transform = list( 
             Playback = ~ readr::parse_factor(.x, ordered=F)), #make factor
         values_drop_na = T #drop NA values
         ) %>%
     mutate(
       Response_twitchplus = if_else(Response.score >=1, 1,0), #if response >= 1, put 1, if less, put 0 (make binary for if they at least twitched or not)
         Response_approachplus = if_else(Response.score >=2, 1,0), #if response >= 2, put 1, if less, put 0 (make binary if they at least approached or not)
        Response_attack =if_else(Response.score >=3, 1,0), #if response 3 put 1, otherwise 0 (make binary for if they at least attacked or not)
    )  %>%
     mutate(
          Days.Between = replace(Days.Between, Days.Between == 0, NA) #For controls bats, "timebetween" should be NA, not 0

     )



# Only 2 groups, and categorize playbacks as Primary, control, static)
LTM2 <- LTM1 %>% 
  filter(Group %in% c("control", "treatment")) %>%
  filter( !Playback %in% c( "A","B"))  %>% droplevels.data.frame()
  
# only 2 groups, and categorize playbacks as A, B, control, and static
LTM_AB <- LTM1 %>% 
  filter(Group %in% c("control", "treatment")) %>%
  filter( !Playback %in% "Primary")  %>% droplevels.data.frame()

```


## Explore data
```{r}
# summarize the number of bats in each group, the # of bats with each ordinal response

#.5 scores are possible bc we average the response of naive bats to sound a and b.
LTM_response <- LTM1  %>% group_by(Group, Playback) %>%
      summarise( n=n() , n_0= sum(Response.score==0),  n_0.5= sum(Response.score==0.5), n_1= sum(Response.score==1),n_1.5= sum(Response.score==1.5), n_2= sum(Response.score==2),n_2.5= sum(Response.score==2.5), n_3= sum(Response.score==3) ) 
# %>%   pivot_wider(names_from = Playback, values_from= n) 
LTM_response


#summarize # bats that at least approached and those that attacked
LTM_approach <- LTM1  %>% group_by(Group, Playback) %>%
      summarise( n=n() , n_noresp= sum(Response.score==0), n_twitchhplus= sum(Response_twitchplus ==1), n_approachplus= sum(Response_approachplus ==1), n_attack = sum(Response_attack==1)) 
LTM_response
LTM_approach
#View(LTM_approach)

```


## Make dataset where response scores to Primary are calculated as the maximum response to A or B, rather than the mean: 
```{r}
# add a column where "primary" is calculated for naive bats by taking their highest response to A or B (rather than the mean response)

#Comparison: If we conservatively calculate "primary" as naive bat's highest score to either A or B, rather than their mean score, does that change the results? 


# pull only the control bats
# pull A and B
#take the higher rows for A and B
# add back
 LTM_test <- LTM1 %>% 
  mutate(Playback.max = Playback) %>% #make new column
  filter(Group %in% "control") %>% #pull control bats
  filter( Playback %in% c("A", "B")) %>% #pull A and B
  group_by(Bat.ID)  %>%
    slice(which.max(Response.score)) %>%#pull out row with higher value
 mutate(Playback.max = "Primary") #change name to primary from A or B
 
 #now combine that with treatment bat primary responses
#remove A and B from treatment, remove A and B and primary from controls
 LTM_playbackmax <- LTM1 %>%
  mutate(Playback.max = Playback) %>%
 filter( !Playback %in% c("A", "B"))  %>% droplevels.data.frame() %>%
  filter(Group == "treatment" | Group == "control" & Playback != "Primary") %>%  #keep all treatment values and remove control values with Primary
   rbind(LTM_test) %>% #add back in the new computation for primary
   select(-Playback) %>%
   arrange(Group,Bat.ID, Playback.max)
 

```


## Functions for bootstrapping 95% confidence intervals around the mean 
```{r}
# LTM tests
# Gerry Carter code

# functions for bootstrapping 95% confidence intervals around the mean -----
d <- LTM2 
# get mean and 95% CI of values x via bootstrapping
boot_ci <- function(x, perms=5000, bca=F) {
  get_mean <- function(x, d) {
    return(mean(x[d]))
  } 
  x <- as.vector(na.omit(x))
  mean <- mean(x)
  if(bca){
    boot <- boot.ci(boot(data=x, 
                         statistic=get_mean, 
                         R=perms, 
                         parallel = "multicore", 
                         ncpus = 4), 
                    type="bca")
    low <- boot$bca[1,4]
    high <- boot$bca[1,5] 
  }else{
    boot <- boot.ci(boot(data=x, 
                         statistic=get_mean, 
                         R=perms, 
                         parallel = "multicore", 
                         ncpus = 4), 
                    type="perc")
    low <- boot$perc[1,4]
    high <- boot$perc[1,5] 
  }
  c(low=low,mean=mean,high=high, N=round(length(x)))
}


# get mean and 95% CI via bootstrapping of values y within grouping variable x
boot_ci2 <- function(d=d, y=d$y, x=d$x, perms=5000, bca=F){
  df <- data.frame(effect=unique(x))
  df$low <- NA
  df$mean <- NA
  df$high <- NA
  df$n.obs <- NA
  for (i in 1:nrow(df)) {
    ys <- y[which(x==df$effect[i])] #pulls out all the e.g.location prefs
    if (length(ys)>1 & var(ys)>0 ){
      b <- boot_ci(y[which(x==df$effect[i])], perms=perms, bca=bca) #resamples with replacement the mean and ci for e.g. AJs pref for location, gives back the low mean and high
      df$low[i] <- b[1]
      df$mean[i] <- b[2]
      df$high[i] <- b[3]
      df$n.obs[i] <- b[4]
    }else{
      df$low[i] <- min(ys)
      df$mean[i] <- mean(ys)
      df$high[i] <- max(ys)
      df$n.obs[i] <- length(ys)
    }
  }
  df
}

# plot permutation test results 
hist_perm <- function(exp=exp, obs=obs, perms=perms, label=''){
  exp.range <- round(quantile(exp, probs= c(0.025, 0.975)),3)
  ggplot()+
    geom_histogram(aes(x=exp), color="black",fill="light blue")+
    geom_vline(aes(xintercept=obs), color="red", size=1)+
    xlab("expected values from null model")+
    ggtitle(label, subtitle = paste('obs = ',round(obs,3), ', exp = ', exp.range[1], ' to ', exp.range[2], ", Prob exp >= obs: p", ifelse(mean(exp>=obs)==0,paste("<",1/perms), paste("=",signif(mean(exp>=obs),digits=2))),", permutations=",perms, sep=""))
}
```


## Bootstap bat responses, plot
```{r}
#Gerry code
plot_preference <- function(OG_scores=T){
  
#  alt method calculates bootstraps using the playback.max scale (test by checking means_control, primary mean should be 1 if using the playback max values)
  
# plot2 within function- values change slightly for two methods bc of the way set.seed works

 # control/ naives
       if(OG_scores){
    points <- 
     d %>% 
      # get group
      #filter(Group=="control") %>% 
      group_by(Bat.ID) %>% 
      mutate(pref = Response.score) %>% 
      ungroup() %>% 
      # identify trial for bootstrapping
      mutate(effect= Playback)  
 
  }else{
    points <- 
     LTM_playbackmax %>% 
      # get group
      #filter(Group=="control") %>% 
      group_by(Bat.ID) %>% 
      mutate(pref = Response.score) %>% 
      ungroup() %>% 
      # identify trial for bootstrapping
      mutate(effect= Playback.max)  
    
    
  }

    
      
#effect = Playback      
  # get means and 95% CI
  set.seed(121)
  means_control <- 
    points %>% 
    filter(Group=="control") %>%
    boot_ci2(y=.$pref, x=.$effect)
    
  
 means_treatment <- points %>% 
       filter(Group=="treatment") %>%
    boot_ci2(y=.$pref, x=.$effect)
 
 means<- rbind(means_control, means_treatment) %>% mutate(Group =c(rep("control", 3), rep("treatment",3)))  #unlist()

 
 # write_csv(means, paste0("confint_", "all", format(Sys.time(), "%Y-%m-%d_%H-%M"), ".csv") )

 


# plot means, 95% CI and raw data
 
 #dodge the minor points for plotting  
  #deffect= playback (x) dodged by treatment
  #jpref = pref (y) dodged by treatment
  points = transform(points, deffect = ifelse(Group == "control", 
                                     as.numeric(effect) - .15,
                                     as.numeric(effect) + .15 ) )
 points = transform(points, deffect = ifelse(Group == "control", 
                                     jitter(as.numeric(effect) - .08, .1),
                                     jitter(as.numeric(effect) + .08, .1) ),
               jpref = jitter(pref, amount = 0) )


 (plot2 <- 
    means %>% 
    ggplot(aes(x=effect, y=mean, color= Group))+
    #facet_wrap(~Group, scales="free")+
    geom_blank(data=points, aes(x=effect, y=pref)) +
    geom_point(size=3, position = position_dodge(width = 0.3))+ #bootstrapped means
   # geom_line(data= points, aes(x=deffect, y= jpref, group= Bat.ID, color =Group),  alpha=0.2) +  #minor points individual bats
   # geom_point( data = points, aes(x= deffect, y=jpref, group=Bat.ID), size=1, alpha=0.1 )  + #minor lines individual bats 
    geom_errorbar(aes(ymin=low, ymax=high, width=.1), size=1, position = position_dodge(width = 0.3))+ #bootstrapped confidence intervals +
        scale_color_manual(values=c('dark grey','#1b9e77','#d95f02','#7570b3'), labels= c("control" = "Naïve", "treatment" = "Experienced"), name = "Treatment") +

    # geom_blank(data=points, aes(x=effect, y=pref)) +
   
   scale_x_discrete(labels= c("Primary" = "Trained", "Control"= "Extinguished", "Static" ="Control" )) +
 
     ylab("Response scores")+
      xlab("")+
   
    theme_cowplot(8)+
   
     scale_y_continuous(labels= c("0" = "No response (0)", "1" = "Ear twitches (1)",
                              "2" = "Approach (2)", "3"= "Attack (3)")) 
 
 )
 
 
 list(means,plot2)
 
}

bootstrapped_OG <- plot_preference(OG_scores = TRUE)
bootstrapped_primarymax <- plot_preference(OG_scores = FALSE)

bootstrapped_OG
bootstrapped_primarymax



```


## Plot of all bat responses, faceted
```{r}
 ## plot of all bats responses, faceted out,  as a barchart


##  add small value to "0" bars for plotting (make 0's visible)
LTM_plotting <- LTM_AB %>% mutate(Response.score, Plotting.scores = if_else(Response.score == "0", .1, Response.score ))
   


 plot5 <- 
     LTM_plotting %>% 
     ggplot(aes(x=Playback, y=Plotting.scores,  fill = Group))+
     facet_wrap(~ Days.Between + Bat.ID, 
               # ncol = 4, # leave out to have default gridding
                strip.position = "top") + #facet by time in wild (helps to order plots)
     geom_bar( position = position_dodge(width = 0.3), stat = "identity")+ #bootstrapped means
    # geom_errorbar(aes(ymin=low, ymax=high, width=.1), size=1, position = position_dodge(width = 0.3))+ #bootstrapped confidence intervals +
         scale_fill_manual(values=c('dark grey','#1b9e77','#d95f02','#7570b3'), labels= c("control" = "Naïve", "treatment" = "Experienced"), name = "Treatment") +
  scale_y_continuous(labels= c("0" = "No response (0)", "1" = "Ear twitches (1)",
                              "2" = "Approach (2)", "3"= "Attack (3)") ) +#, expand = expand_scale(add = .5) ) + #adds padding at top and bottom of y axis
    # 
    #  geom_blank(data=points, aes(x=effect, y=pref)) +
    
    #geom_point( data = points, aes(x= deffect, y=jpref, group=Bat.ID), size=1, alpha=0.1 )  +
    
   #geom_line(data= LTM_AB, aes(x=Playback, y= Plotting.scores, group= Bat.ID, color =Group),  alpha=0.5) + 
   scale_x_discrete(labels= c("A" = "A", "B" = "B", "Control"= "E", "Static" ="C" )) +
 
     ylab("Highest Response")+
      xlab("")+
   
   #geom_text(aes(label = Days.Between, x =  0.45, y = 0, vjust = 0.4, hjust = "left",),  show.legend = FALSE)+ 
    #theme_cowplot(8)+
   #  theme_minimal() +
   theme(
    strip.text.x = element_blank() ,
    #panel.grid = element_blank(), 
    panel.border = element_rect(color ="black", fill = NA), 
    #panel.grid.major.y = element_line(color = "gray"),
    panel.grid.minor = element_blank(),
    # plot.background = element_blank(),
    axis.ticks.x = element_line(color = "black"),
    axis.ticks.y = element_line(color = "black"),
    panel.spacing = unit(10, "pt"),
    legend.position = "top" )   #legend on top
   
     
 
plot5
#ggsave("ResponsesAB_bybat_barplot.pdf", width= 5, height = 4) #6 x 4.5 looks okay



 
```

# Did naive bats respond differently than experienced bats?
Permutation approach- use difference between means of groups.

## Did experienced and naive bats respond differently to the different sounds?
```{r}
# Are there treatment differences?
# use permutation test to compare experienced and naive bat responses to the three sound treatments (using null model that randomizes treatment group while controlling for everything else)


# get difference between groups in mean response to each playback
# store as vector of differences
set.seed(122)

obs<- LTM2 %>% 
  ungroup() %>%
  group_by(Group, Playback) %>% 
  summarize(response= mean(Response.score)) %>% 
  pivot_wider(names_from = Group, values_from= response) %>% 
  mutate(diff= treatment-control) %>% 
  arrange(Playback) %>% 
  pull(diff)

# numbers are T-C differences for:
# primary control static
# positive number means treatments had higher score
# we cannot interpret these without the null model of what to expect by chance!


obs

# get expected difference between treatment if bat choices were random across treatments 

# how many permutations?
perms <- 5000

# store results in matrix
exp <- matrix(NA, ncol=length(obs), nrow= perms) 


 namesgroup<- LTM2 %>%
    ungroup() %>%
    distinct(Bat.ID, .keep_all=T) %>%
    select(Bat.ID, Group)
  
# for loop to get expected results
  for(i in 1:perms) {
 
 #pull out unique ids, randomly reassign group to them, add back 
 Groupperm <- namesgroup %>% mutate(permgrp = sample(Group)) %>% select(Bat.ID, permgrp)
  LTM_perm <- LTM2 %>% left_join(Groupperm)

  # get same number as before, but with permuted group values
  exp[i,] <- 
    LTM_perm %>% 
    group_by(permgrp, Playback) %>% 
    summarize(response= mean(Response.score), .groups= 'drop') %>% #  # apparently drop is needed just bc new version of tidyverse gives a random warning, this prevents that  
  pivot_wider(names_from = permgrp, values_from= response) %>% 
  mutate(diff= treatment-control) %>% 
  arrange(Playback) %>% 
  pull(diff)
  } 
  
# get p-value for species difference for primary (column 1)
hist_perm(exp= exp[,1], obs= obs[1], perms=perms) 
#ggsave("histogram_permutations_experienced_v_naive_trained.jpg")
# get p-value for species difference for control/ extinguished (column 2)
hist_perm(exp= exp[,2], obs= obs[2], perms=perms) 
#ggsave("histogram_permutations_experienced_v_naive_extinguished.jpg")
# get p-value for species difference for static (column 3)
hist_perm(exp= exp[,3], obs= obs[3], perms=perms)
#ggsave("histogram_permutations_experienced_v_naive_staticcontrol.jpg")

###
  ###
 # p value for primary/ trained: 
sum((exp[,1] >= obs[1])/perms) #one tailed #0 (or rather, the 1/ # of perms)
sum((abs(exp[,1]) >= abs(obs[1]))/perms) #two-tailed #0

## p value for control/ extinguished
sum((exp[,2] >= obs[2])/perms) #0
sum((abs(exp[,2]) >= abs(obs[2]))/perms) #two-tailed #0
 
 ## p value for static/ control
 sum((exp[,1] >= obs[3])/perms) #1204
 sum((abs(exp[,3]) >= abs(obs[3]))/perms) #two-tailed 0.2252
  
 beep(5)
```


## Did experienced bats respond differently to the trained and control sounds? 
```{r}
# Did experienced bats respond differently to primary/trained vs static/control treatments?
# use permutation test to compare treatments (using null model that randomizes treatment group while controlling for everything else)


# get difference between treatments in mean response to each playback
# store as vector of differences

#only 1 group, relevant playbacks
LTM3 <- LTM1 %>% 
  filter(Group %in% c( "treatment")) %>%
  filter( !Playback %in% c( "A","B"))  %>% droplevels.data.frame()
  

obs<- LTM3 %>% 
  ungroup() %>%
  group_by(Playback) %>% 
  summarize(response= mean(Response.score)) %>% 
  pivot_wider(names_from = Playback, values_from= response) %>% 
  mutate(diff= Primary-Static) %>% 
  pull(diff)

# number is difference for:
# primary and static
# positive number means primary had higher score
# we cannot interpret these without the null model of what to expect by chance!


obs

# get expected difference between treatment if bat choices were random across treatments 

# how many permutations?
perms <- 5000

# store results in matrix
exp <- matrix(NA, ncol=length(obs), nrow= perms) 

##stopped here, I want to pull out distinct bats, randomly reassign the playback, add them back
# #pull out distinct names (changed Group to Playback)
#   namesgroup<- LTM3 %>%
#     ungroup() %>%
#     distinct(Bat.ID, .keep_all=T) %>%
#     select(Bat.ID, Group)
  
# for loop to get expected results
  for (i in 1:perms){
  
  exp[i,] <- 
    LTM3 %>% 
    # re-assign visits to different cues within each trial 
    group_by(Bat.ID) %>% 
    mutate(Response.score = Response.score[sample(row_number())]) %>%  
    
  ungroup() %>%
  group_by(Playback) %>% 
  summarize(response= mean(Response.score)) %>% 
  pivot_wider(names_from = Playback, values_from= response) %>% 
  mutate(diff= Primary-Static) %>% 
  pull(diff)
   #beep(12)  
} 
# get p-value for difference in experienced bat response between primary/trained and control/static: 
hist_perm(exp= exp[,1], obs= obs[1], perms=perms)




###
  ###
 # p value for difference in experienced bat response to primary/trained and control/static: 
sum((exp[,1] >= obs[1])/perms) #one tailed, 0
sum((abs(exp[,1]) >= abs(obs[1]))/perms) #two-tailed, 0

#best to use two-tailed, bc would be interesting/ possible if bats had higher responses to static
```

## Did experienced bats respond differently to the extinguished and control sounds?
```{r}
# Did experienced bats respond differently to the extinguished/control vs control/static treatments? 

# use permutation test to compare treatments (using null model that randomizes treatment group while controlling for everything else)


# get difference between treatments in median response to each playback
# store as vector of differences

#only 1 group, relevant playbacks
LTM3 <- LTM1 %>% 
  filter(Group %in% c( "treatment")) %>%
  filter( !Playback %in% c( "A","B"))  %>% droplevels.data.frame()
  

obs<- LTM3 %>% 
  ungroup() %>%
  group_by(Playback) %>% 
  summarize(response= mean(Response.score)) %>% 
  pivot_wider(names_from = Playback, values_from= response) %>% 
  mutate(diff= Control-Static) %>% 
  pull(diff)

# number is difference for:
# primary and static
# positive number means primary had higher score
# we cannot interpret these without the null model of what to expect by chance!


obs

# get expected difference between treatment if bat choices were random across treatments 

# how many permutations?
perms <- 5000

# store results in matrix
exp <- matrix(NA, ncol=length(obs), nrow= perms) 

##stopped here, I want to pull out distinct bats, randomly reassign the playback, add them back
# #pull out distinct names (changed Group to Playback)
#   namesgroup<- LTM3 %>%
#     ungroup() %>%
#     distinct(Bat.ID, .keep_all=T) %>%
#     select(Bat.ID, Group)
  
# for loop to get expected results
  for (i in 1:perms){
  
  exp[i,] <- 
    LTM3 %>% 
    # re-assign visits to different cues within each trial 
    group_by(Bat.ID) %>% 
    mutate(Response.score = Response.score[sample(row_number())]) %>%  
    
  ungroup() %>%
  group_by(Playback) %>% 
  summarize(response= mean(Response.score)) %>% 
  pivot_wider(names_from = Playback, values_from= response) %>% 
  mutate(diff= Control-Static) %>% 
  pull(diff)
     
} 
# get p-value for difference in experienced bat response between extinguished and static: 
hist_perm(exp= exp[,1], obs= obs[1], perms=perms)




###
  ###
 # p value for difference in experienced bat response to extinguished and static: 
sum((exp[,1] >= obs[1])/perms) #one tailed 0
sum((abs(exp[,1]) >= abs(obs[1]))/perms) #two-tailed 0

#best to use two-tailed, bc would be interesting/ possible

  beep(3)
```


## Difference between trained and extinguished? 
```{r}
# Did experienced bats respond differently to the trained vs extinguished treatments?

# use permutation test to compare treatments (using null model that randomizes treatment group while controlling for everything else)


# get difference between treatments in median response to each playback
# store as vector of differences

#only 1 group, relevant playbacks
LTM3 <- LTM1 %>% 
  filter(Group %in% c( "treatment")) %>%
  filter( !Playback %in% c( "A","B"))  %>% droplevels.data.frame()
  

obs<- LTM3 %>% 
  ungroup() %>%
  group_by(Playback) %>% 
  summarize(response= mean(Response.score)) %>% 
  pivot_wider(names_from = Playback, values_from= response) %>% 
  mutate(diff= Primary-Control) %>% 
  pull(diff)

# number is difference for:
# primary and static
# positive number means primary had higher score
# we cannot interpret these without the null model of what to expect by chance!


obs

# get expected difference between treatment if bat choices were random across treatments 

# how many permutations?
perms <- 5000

# store results in matrix
exp <- matrix(NA, ncol=length(obs), nrow= perms) 

##stopped here, I want to pull out distinct bats, randomly reassign the playback, add them back
# #pull out distinct names (changed Group to Playback)
#   namesgroup<- LTM3 %>%
#     ungroup() %>%
#     distinct(Bat.ID, .keep_all=T) %>%
#     select(Bat.ID, Group)
  
# for loop to get expected results
  for (i in 1:perms){
  
  exp[i,] <- 
    LTM3 %>% 
    # re-assign visits to different cues within each trial 
    group_by(Bat.ID) %>% 
    mutate(Response.score = Response.score[sample(row_number())]) %>%  
    
  ungroup() %>%
  group_by(Playback) %>% 
  summarize(response= mean(Response.score)) %>% 
  pivot_wider(names_from = Playback, values_from= response) %>% 
  mutate(diff= Primary-Control) %>% 
  pull(diff)
   #beep(12)  
} 
# get p-value for difference in experienced bat response between trained and extinguished: 
hist_perm(exp= exp[,1], obs= obs[1], perms=perms)




###
  ###
 # p value for difference in experienced bat response to trained and extinguished: 
sum((exp[,1] >= obs[1])/perms) #one tailed
sum((abs(exp[,1]) >= abs(obs[1]))/perms) #two-tailed

#best to use two-tailed, bc would be interesting/ possible
  
```


# Test, same stats, but with conservative value for naive bats response to primary/ trained (playbackmax)
See if calculating the response to the trained stimulus using the maximum response to A and B, rather than the mean, makes any difference to the results

## Summarize any differences between the two primary calculation  methods. Other values should be same, Response scores for naive primary should be slightly different.
```{r}

beep(1)
#comparing naive trained responses with two methods side by side. 

#pulling out Response.scores for controls to Primary with both methods
LTM_compare <-LTM_playbackmax %>% 
  filter(Group=="control", Playback.max =="Primary") %>% 
  select(Response.score)%>% 
  rename(Response.score.max = Response.score) %>% 
  cbind(LTM2 %>% filter(Group=="control", Playback =="Primary") %>% 
  select(Response.score) ) 
LTM_compare

#Summarize differences
mean_compare <- LTM_compare %>%
  summarize(mean.response.score.max = mean(Response.score.max), 
            mean.response.score = mean(Response.score), 
            difference = mean(Response.score.max-mean(Response.score)) )
mean_compare
#Difference in the two metrics is 0.205
```

## Permutation test with primarymax scale 
See if calculating the response to the trained stimulus using the maximum response to A and B, rather than the mean, makes any difference to the results
```{r}
# Are there Treatment differences?
# use permutation test to compare treatments (using null model that randomizes treatment group while controlling for everything else)


# get difference between groups in mean response to each playback
# store as vector of differences

#only 2 groups, relevant playbacks


obs<- LTM_playbackmax %>% 
  ungroup() %>%
  group_by(Group, Playback.max) %>% 
  summarize(response= mean(Response.score)) %>% 
  pivot_wider(names_from = Group, values_from= response) %>% 
  mutate(diff= treatment-control) %>% 
  arrange(Playback.max) %>% 
  pull(diff)

# numbers are T-C differences for:
# primary control static
# positive number means treatments had higher score
# we cannot interpret these without the null model of what to expect by chance!

set.seed(122)
obs

# get expected difference between treatment if bat choices were random across treatments 

# how many permutations?
perms <- 5000

# store results in matrix
exp <- matrix(NA, ncol=length(obs), nrow= perms) 


 namesgroup<- LTM_playbackmax %>%
    ungroup() %>%
    distinct(Bat.ID, .keep_all=T) %>%
    select(Bat.ID, Group)
  
# for loop to get expected results
  for(i in 1:perms) {
 
 #pull out unique ids, randomly reassign group to them, add back 
 Groupperm <- namesgroup %>% mutate(permgrp = sample(Group)) %>% select(Bat.ID, permgrp)
  LTM_perm <- LTM_playbackmax %>% left_join(Groupperm)

  # get same number as before, but with permuted group values
  exp[i,] <- 
    LTM_perm %>% 
    group_by(permgrp, Playback.max) %>% 
    summarize(response= mean(Response.score), .groups= 'drop') %>% #  # apparently drop is needed just bc new version of tidyverse gives a random warning, this prevents that  
  pivot_wider(names_from = permgrp, values_from= response) %>% 
  mutate(diff= treatment-control) %>% 
  arrange(Playback.max) %>% 
  pull(diff)
  } 
  
# get p-value for species difference for primary/trained (column 1)
hist_perm(exp= exp[,1], obs= obs[1], perms=perms) 
#ggsave("histogram_permutations_experienced_v_naive_trained.jpg")
# get p-value for species difference for control/ extinguished (column 2)
hist_perm(exp= exp[,2], obs= obs[2], perms=perms) 
#ggsave("histogram_permutations_experienced_v_naive_extinguished.jpg")
# get p-value for species difference for static (column 3)
hist_perm(exp= exp[,3], obs= obs[3], perms=perms)
#ggsave("histogram_permutations_experienced_v_naive_staticcontrol.jpg")

###
  ###
 # p value for primary/ trained: 
sum((exp[,1] >= obs[1])/perms) #one tailed #0
sum((abs(exp[,1]) >= abs(obs[1]))/perms) #two-tailed #0 # still the same result

## p value for control/ extinguished
sum((exp[,2] >= obs[2])/perms) #one tailed # 2e-4
sum((abs(exp[,2]) >= abs(obs[2]))/perms) #two-tailed #2e-4
 
 ## p value for static/ control
 sum((exp[,1] >= obs[3])/perms) #0.084
 sum((abs(exp[,3]) >= abs(obs[3]))/perms) #two-tailed 0.2162
  beep(8)
```
